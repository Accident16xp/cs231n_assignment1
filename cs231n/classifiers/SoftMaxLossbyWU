import numpy as np
from builtins import range
from past.builtins import xrange
from random import shuffle

def softmax_loss_naive(W,X,y,reg):
    num_classes=W.shape[1]
    num_train=W.shape[0]
    loss=0.0
    dW=np.zeros(W.shape)
    
    for i in xrange(num_train):
        scores=X[i].dot(W)
        exp_scores=np.zeros(W.shape)
        correct_class=y[i]
        sumofrow=0
        #下面要计算softmax损失
        for j in xrange(num_train):
            exp_scores[j]=np.exp(scores[j])
            sumofrow+=exp_scores[j]
            #损失表示为：指数项/指数项之和
            loss+=-np.log(exp_scores[correct_class]/row_sum)
            for k in xrange(num_classes):
                if k!=correct_class:
                    #误分类情形
                    dW[:,k]+=exp_scores[k]/row_sum*X[i]
                else:
                    dW[:,k]+=(exp_scores[correct_class]/row_sum-1)*X[i]
    loss/=num_train
    loss+=0.5*reg*np.sum(W**2)
    dW/=num_train
    dW+=reg*W
    return loss,dW


#再写一个向量化实现的版本：
def softmax_loss_vectorized(W,X,y,reg):
    num_train=X.shape[0]
    num_classes=X.shape[1]
    loss=0.0
    dW=np.zeros(W.shape)
    scores=X.dot(W)
    exp_scores=np.exp(scores)
    #分母
    sumofrow=exp.scores.sum(axis=1)
    #review axis=0,按列加，axis=1按行加
    sumofrow=sumofrow.reshape((num.train,1))
    #将损失归一化
    norm_exp=exp_scores/sumofrow
    row_index=np.arange(num_train)
    data_loss=norm_exp[row_index,y].sum()
    
    loss=data_loss/num_train+0.5*reg*np.sum(W**2)
    #对于正确的多减了一项1
    norm_exp[row_index,y]-=1
    
    dW=X.T.dot(norm_exp)
    dW/=num_train
    dW+=reg*W
   
return loss,dW
    
                 
            
            
        
        